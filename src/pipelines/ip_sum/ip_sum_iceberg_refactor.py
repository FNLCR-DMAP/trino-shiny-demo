#!/usr/bin/env python3
"""
Clean IP Sum pipeline using the simplified data_utils API.
This üöÄ CONFIGURATION-DRIVEN DATA ENGINEERING API:emonstrates how data engineers can focus on business logic while
data_utils handles lineage, context detection, and platform operations.
"""

import sys
import os
from pyspark.sql.functions import current_timestamp, lit

# Import the simplified data utilities
sys.path.append(os.path.join(os.path.dirname(__file__), '../../dmap_data_sdk'))
from data_utils import DataPipeline, Ref

def ip_sum(df):
    """
    Your original transformation logic - UNCHANGED
    This is your existing business logic that processes the Full_Name data
    """
    # Your existing transformation code goes here
    # For demonstration, I'll show a simple example:
    from pyspark.sql.functions import regexp_replace, size, split, length, when, col
    
    result = df.select(
        col(df.columns[0]).alias("id") if len(df.columns) > 0 else lit(1).alias("id"),
        col(df.columns[1]).alias("original_full_name") if len(df.columns) > 1 else lit("").alias("original_full_name"),
        regexp_replace(col(df.columns[1]) if len(df.columns) > 1 else lit(""), r"[^a-zA-Z\s]", "").alias("processed_full_name"),
        size(split(col(df.columns[1]) if len(df.columns) > 1 else lit(""), "\\s+")).alias("name_parts_count"),
        current_timestamp().alias("processing_timestamp")
    )
    
    return result

def main(argv):
    """
    ULTRA-CLEAN IP Sum pipeline! 
    DataPipeline handles Spark session creation, Iceberg configuration, 
    context detection, and lineage tracking automatically.
    """
    if len(argv) < 2:
        print("Usage:")
        print("  python ip_sum_iceberg.py <input_table> <output_table> [branch]")
        print("Examples:")
        print("  python ip_sum_iceberg.py iceberg.data_pipeline.full_name_input iceberg.data_pipeline.ip_sum_output")
        print("  python ip_sum_iceberg.py input_table output_table feature_branch")
        print("  # Airflow context auto-detected when running in Airflow")
        sys.exit(1)
    
    input_table = argv[0]
    output_table = argv[1]
    
    try:
        # Initialize pipeline from configuration - handles EVERYTHING automatically:
        # ‚úÖ Platform detection from config file (Iceberg/Delta/Snowflake)
        # ‚úÖ Spark session with platform-specific configuration  
        # ‚úÖ Context detection (Airflow vs standalone)
        # ‚úÖ Lineage tracking setup
        # ‚úÖ Error handling and graceful fallbacks
        pipeline = DataPipeline.from_config(transform_name="ip_sum")
        
        # Optional: Override branch from command line (for manual testing)
        if len(argv) > 2:
            branch = argv[2]
            pipeline.set_input_ref(Ref(branch=branch))
            pipeline.set_target_ref(Ref(branch=branch))
        
        # Business logic (only 3 lines needed!):
        df_input = pipeline.read_table(input_table)     # Auto-tracked lineage
        result_df = ip_sum(df_input)                    # Your transform logic  
        write_result = pipeline.write_table(result_df, output_table)  # Auto-recorded lineage
        
        print("‚úÖ Pipeline completed successfully!")
        print("üéØ DataPipeline automatically handled:")
        print(f"   ‚Ä¢ Platform configuration from config file ({pipeline.platform_type})")
        print("   ‚Ä¢ Context detection (Airflow vs standalone)")
        print("   ‚Ä¢ Complete lineage tracking")
        print("   ‚Ä¢ Branch/time travel support")
        print("   ‚Ä¢ Snapshot metadata stamping")
        print("   ‚Ä¢ Error handling and graceful fallbacks")
        
        return write_result
        
    except Exception as e:
        print(f"‚ùå Pipeline failed: {e}")
        raise

if __name__ == '__main__':
    main(sys.argv[1:])

"""
ÔøΩ CLEAN DATA ENGINEERING API with data_utils:

BEFORE (Complex boilerplate):
- Manual context detection
- Manual lineage tracking  
- Manual platform configuration
- Manual error handling
- 100+ lines of setup code

AFTER (Clean & Simple):
- pipeline = DataPipeline(spark, "transform_name")  # Auto-detects everything
- df = pipeline.read_table("input_table")           # Auto-tracks lineage
- result = pipeline.write_table(df, "output_table") # Auto-records lineage

USAGE EXAMPLES:

# Basic usage (auto-detects context, uses main branch):
python ip_sum_iceberg_refactor.py iceberg.data_pipeline.full_name_input iceberg.data_pipeline.ip_sum_output

# Manual branch override (for testing):
python ip_sum_iceberg_refactor.py input_table output_table feature_branch

AIRFLOW INTEGRATION:
When running in Airflow, the DataPipeline automatically:
‚úÖ Detects DAG/Task/Run IDs from environment
‚úÖ Handles branch/time travel via Airflow params
‚úÖ Records complete lineage with run context
‚úÖ Stamps snapshots with execution metadata

LINEAGE QUERIES (via Trino):
-- View pipeline history
SELECT * FROM audit.etl_lineage WHERE transform = 'ip_sum' 
ORDER BY recorded_at DESC LIMIT 10;

-- Trace data dependencies  
SELECT inputs_json FROM audit.etl_lineage 
WHERE target_table = 'iceberg.data_pipeline.ip_sum_output'
  AND target_snapshot_id = 12345;

-- Branch analytics
SELECT code_branch, COUNT(*) as runs, 
       MAX(recorded_at) as last_run 
FROM audit.etl_lineage 
WHERE transform = 'ip_sum' 
GROUP BY code_branch;

DATA ENGINEER BENEFITS:
‚úÖ 90% less boilerplate code
‚úÖ Zero manual lineage tracking  
‚úÖ Automatic context detection
‚úÖ Built-in error handling
‚úÖ Focus on business logic only
"""