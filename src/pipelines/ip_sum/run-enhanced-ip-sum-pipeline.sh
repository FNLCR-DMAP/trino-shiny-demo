#!/bin/bash

echo "üîÑ Running Enhanced IP Sum Pipeline with Configuration-Driven DataPipeline"
echo "This runs the refactored ip_sum_iceberg_refactor.py with data_utils in Spark container"
echo ""

# Configuration
SCHEMA_NAME=${1:-"data_pipeline"}
INPUT_TABLE="iceberg.${SCHEMA_NAME}.full_name_input"
OUTPUT_TABLE="iceberg.${SCHEMA_NAME}.ip_sum_output"
PYTHON_SCRIPT="ip_sum_iceberg_refactor.py"
DATA_UTILS_MODULE="data_utils.py"
CONFIG_FILE="pipeline_config.yaml"

# Step 1: Ensure infrastructure is running
echo "1. Checking infrastructure..."
if ! docker ps | grep -q "spark-iceberg"; then
    echo "   ‚ö†Ô∏è  Spark container not running. Starting infrastructure..."
    docker-compose up -d
    echo "   ‚Üí Waiting for services to be ready..."
    sleep 15
else
    echo "   ‚úì Infrastructure is running"
fi

# Step 2: Verify input data exists
echo ""
echo "2. Verifying input data..."
INPUT_COUNT=$(docker exec trino-cli trino --server trino:8080 --user admin --execute "SELECT COUNT(*) FROM ${INPUT_TABLE}" 2>/dev/null | tail -1 | tr -d '"')

if [[ ! -z "$INPUT_COUNT" && "$INPUT_COUNT" -gt "0" ]]; then
    echo "   ‚úì Input data verified: ${INPUT_COUNT} records in ${INPUT_TABLE}"
else
    echo "   ‚ùå No input data found. Run 'make init-ip-sum-data' first"
    exit 1
fi

# Step 3: Install PyYAML in container if needed
echo ""
echo "3. Ensuring dependencies..."
echo "   ‚Üí Checking PyYAML availability..."
docker exec spark-iceberg python -c "import yaml; print('PyYAML available')" 2>/dev/null || {
    echo "   ‚Üí Installing PyYAML..."
    docker exec spark-iceberg pip install pyyaml
}
echo "   ‚úì Dependencies ready"

# Step 4: Copy required files to Spark container
echo ""
echo "4. Preparing transformation files..."
echo "   ‚Üí Copying enhanced pipeline script..."
docker cp src/pipelines/ip_sum/${PYTHON_SCRIPT} spark-iceberg:/opt/spark/work-dir/${PYTHON_SCRIPT}
echo "   ‚Üí Copying data_utils module..."
docker cp src/dmap_data_sdk/${DATA_UTILS_MODULE} spark-iceberg:/opt/spark/work-dir/${DATA_UTILS_MODULE}
echo "   ‚Üí Copying configuration file..."
docker cp ${CONFIG_FILE} spark-iceberg:/opt/spark/work-dir/${CONFIG_FILE}
echo "   ‚úì All files copied successfully"

# Step 5: Fix import paths for container execution
echo ""
echo "5. Preparing container environment..."
docker exec spark-iceberg sed -i 's|sys.path.append.*|# Import directly from current directory|' /opt/spark/work-dir/${PYTHON_SCRIPT}
echo "   ‚úì Import paths configured for container"

# Step 6: Run the enhanced transformation
echo ""
echo "6. Running Enhanced IP Sum transformation..."
echo "   ‚Üí Input:  ${INPUT_TABLE}"
echo "   ‚Üí Output: ${OUTPUT_TABLE}"
echo "   ‚Üí Configuration: ${CONFIG_FILE}"
echo "   ‚Üí Processing with configuration-driven DataPipeline..."

TRANSFORM_RESULT=$(docker exec spark-iceberg /opt/spark/bin/spark-submit \
    --jars /opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.4.2.jar \
    --py-files /opt/spark/work-dir/${DATA_UTILS_MODULE} \
    /opt/spark/work-dir/${PYTHON_SCRIPT} ${INPUT_TABLE} ${OUTPUT_TABLE} 2>&1)

if [[ $? -eq 0 ]]; then
    echo "   ‚úì Enhanced transformation completed successfully"
    echo ""
    echo "üìä Transformation Output:"
    echo "$TRANSFORM_RESULT"
else
    echo "   ‚ùå Transformation failed"
    echo "$TRANSFORM_RESULT"
    exit 1
fi

# Step 7: Verify results with cross-engine query (Trino)
echo ""
echo "7. Verifying results with Trino (cross-engine verification)..."

# Check output count
OUTPUT_COUNT=$(docker exec trino-cli trino --server trino:8080 --user admin --execute "SELECT COUNT(*) FROM ${OUTPUT_TABLE}" 2>/dev/null | tail -1 | tr -d '"')

if [[ ! -z "$OUTPUT_COUNT" && "$OUTPUT_COUNT" -gt "0" ]]; then
    echo "   ‚úì Results verified: ${OUTPUT_COUNT} records in ${OUTPUT_TABLE}"
else
    echo "   ‚ö†Ô∏è  Could not verify results"
fi

# Show sample results
echo ""
echo "8. Sample transformation results:"
echo "   ‚Üí Processed names sample:"
docker exec trino-cli trino --server trino:8080 --user admin --execute "
SELECT 
    id,
    original_full_name,
    processed_full_name,
    name_parts_count
FROM ${OUTPUT_TABLE} 
ORDER BY id 
LIMIT 5" 2>/dev/null

# Step 8: Check lineage tracking (if audit table exists)
echo ""
echo "9. Enhanced Features Verification:"
echo "   ‚Üí Checking lineage tracking..."
LINEAGE_COUNT=$(docker exec trino-cli trino --server trino:8080 --user admin --execute "SELECT COUNT(*) FROM audit.etl_lineage WHERE transform = 'ip_sum'" 2>/dev/null | tail -1 | tr -d '"')

if [[ ! -z "$LINEAGE_COUNT" && "$LINEAGE_COUNT" -gt "0" ]]; then
    echo "   ‚úÖ Lineage tracking verified: ${LINEAGE_COUNT} lineage records"
    echo "   ‚Üí Latest lineage record:"
    docker exec trino-cli trino --server trino:8080 --user admin --execute "
    SELECT recorded_at, run_id, target_table, target_snapshot_id 
    FROM audit.etl_lineage 
    WHERE transform = 'ip_sum' 
    ORDER BY recorded_at DESC 
    LIMIT 1" 2>/dev/null
else
    echo "   üìù No lineage records found (audit schema may not exist)"
fi

# Step 9: Demonstrate Iceberg features
echo ""
echo "10. Iceberg Features Demonstration:"

# Time travel - show snapshots
echo "    ‚Üí Available snapshots (time travel capability):"
docker exec trino-cli trino --server trino:8080 --user admin --execute "
SELECT committed_at, operation, summary 
FROM ${OUTPUT_TABLE}\$snapshots 
ORDER BY committed_at DESC 
LIMIT 3" 2>/dev/null

# Metadata tables
echo "    ‚Üí Data files information:"
docker exec trino-cli trino --server trino:8080 --user admin --execute "
SELECT file_format, record_count, file_size_in_bytes 
FROM ${OUTPUT_TABLE}\$files 
LIMIT 3" 2>/dev/null

echo ""
echo "‚úÖ Enhanced IP Sum Pipeline Transformation Complete!"
echo ""
echo "üéØ Enhanced Features Summary:"
echo "   ‚Ä¢ Configuration-driven platform selection (${CONFIG_FILE})"
echo "   ‚Ä¢ Automatic context detection (Airflow vs standalone)"
echo "   ‚Ä¢ Built-in lineage tracking and recording"
echo "   ‚Ä¢ Branch/time travel support"
echo "   ‚Ä¢ Clean data engineering API"
echo ""
echo "üìä Processing Summary:"
echo "   ‚Ä¢ Input records processed: ${INPUT_COUNT}"
echo "   ‚Ä¢ Output records created: ${OUTPUT_COUNT}"
echo "   ‚Ä¢ Lineage records: ${LINEAGE_COUNT}"
echo ""
echo "üîç Query Examples:"
echo "   ‚Ä¢ Current data: SELECT * FROM ${OUTPUT_TABLE}"
echo "   ‚Ä¢ Time travel:  SELECT * FROM ${OUTPUT_TABLE} FOR TIMESTAMP AS OF TIMESTAMP '2025-11-04 12:00:00'"
echo "   ‚Ä¢ Lineage:      SELECT * FROM audit.etl_lineage WHERE transform = 'ip_sum'"
echo ""
echo "üåê Access Points:"
echo "   ‚Ä¢ Trino Web UI: http://localhost:8081"
echo "   ‚Ä¢ Shiny Frontend: http://localhost:8000"